version: "3.6"

x-spark_build: &spark_build
  context: .
  dockerfile: spark.Dockerfile
  args:
    - HADOOP_VERSION=${HADOOP_VERSION}
    - SPARK_VERSION=${SPARK_VERSION}

x-spark_environment: &spark_env
  SPARK_MASTER_HOST: ${SPARK_MASTER_HOST}
  SPARK_MASTER_PORT: ${SPARK_MASTER_PORT}
  PYSPARK_PYTHON: ${PYSPARK_PYTHON}
  SPARK_HOME: /usr/bin/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}

x-spark_worker_environment: &spark_worker_env
  <<: *spark_env
  SPARK_WORKER_CORES: 1
  SPARK_WORKER_MEMORY: 2g

x-networks: &shared_net
  - cluster_network

services:
  client:
    image: tvillani/grandatachallenge:client
    build:
      context: .
      dockerfile: client.Dockerfile
      args:
        - WORKSPACE_PATH=${WORKSPACE_PATH}
        - SPARK_VERSION=${SPARK_VERSION}
        - PYTHON_VERSION=${PYTHON_VERSION}
    container_name: client
    networks: *shared_net
    ports:
      - 8888:8888
      - 4040:4040
    volumes:
      - shared_fs:${WORKSPACE_PATH}

  spark-master:
    image: tvillani/grandatachallenge:spark
    build:
      context: .
      dockerfile: spark.Dockerfile
      args:
        - HADOOP_VERSION=${HADOOP_VERSION}
        - SPARK_VERSION=${SPARK_VERSION}
    container_name: spark-master
    networks: *shared_net
    command: "bin/spark-class org.apache.spark.deploy.master.Master >> logs/spark-master.out"
    ports:
      - 8080:8080
      - 7077:7077
      - 18080:18080
    volumes:
      - shared_fs:${WORKSPACE_PATH}
    environment: *spark_env

  spark-worker-1:
    image: tvillani/grandatachallenge:spark
    build: *spark_build
    container_name: spark-worker-1
    depends_on:
      - spark-master
    networks: *shared_net
    ports:
      - 8081:8081
    volumes:
      - shared_fs:${WORKSPACE_PATH}
    environment: *spark_worker_env
    command: >
      bin/spark-class org.apache.spark.deploy.worker.Worker 
      spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT} >> logs/spark-worker.out

  spark-worker-2:
    image: tvillani/grandatachallenge:spark
    build: *spark_build
    container_name: spark-worker-2
    networks: *shared_net
    ports:
      - 8082:8081
    volumes:
      - shared_fs:${WORKSPACE_PATH}
    environment: *spark_worker_env
    command: >
      bin/spark-class org.apache.spark.deploy.worker.Worker 
      spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT} >> logs/spark-worker.out
    depends_on:
      - spark-master

volumes:
  shared_fs:
    driver: local
    driver_opts:
       o: bind
       type: none
       device: '${PWD}/workspace'

networks:
  cluster_network: